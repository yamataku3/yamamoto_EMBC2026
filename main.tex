\documentclass[letterpaper,10pt,conference]{ieeeconf}
\IEEEoverridecommandlockouts
\overrideIEEEmargins

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}[dvipdfmx]
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{balance}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{color}
\usepackage{textcomp}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\title{\LARGE \textbf{Blurry Class-Incremental Learning for IMU-Based Human Activity Recognition: An Empirical Study}}

\author{Takumi Yamamoto, Suguru Kanoga, Mitsunori Tada, Yuta Sugiura%
\thanks{*This work was partially supported by JSPS KAKENHI Grant-in-Aid for Scientific Research~(B) Grant Number JP23K28135, and Programs for Bridging the gap between R\&{D} and the IDeal society (society $5.0$) and Generating Economic and social value (BRIDGE)/Practical Global Research in the AI $\times$ Robotics Services, implemented by the Cabinet Office, Government of Japan.}
\thanks{T.~Yamamoto and Y.~Sugiura are with Keio University, Kanagawa, Japan (E-mail: {\tt\footnotesize imuka06x17@keio.jp}).}%
\thanks{T.~Yamamoto, S.~Kanoga, and M.~Tada are with the National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan.}%
}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
\maketitle

\begin{abstract}
\textcolor{red}{Inertial measurement unit (IMU)-based human activity recognition (HAR) has attracted considerable attention, leading to a growing demand for systems that support long-term deployment.
In such scenarios, user requirements may evolve over time, necessitating the ability to recognize additional activity classes.
Class-incremental learning (CIL) offers a promising approach by enabling models to incorporate new classes without retraining from scratch.
Although previous studies have examined CIL in the context of HAR, they have largely overlooked cases where the same classes reappear across different tasks--a setting known as the Blurry class-incremental learning (B-CIL) scenario.
In this work, we investigate the B-CIL scenario for IMU-based HAR and conduct extensive experiments on two widely used IMU datasets (UCI-HAR and USC-HAD).
We evaluate nine continual learning methods under multiple configurations of overlapping classes.
Our results demonstrate that replay-based methods consistently outperform regularization-based methods in the B-CIL scenario. 
Furthermore, we observe that increasing the number of overlapping classes can lead to improved performance.
In the future, we aim to extend our study to additional datasets and explore more realistic blurry scenarios, including online continual learning.}
\end{abstract}


\section{Introduction}
\label{sec:introduction}
\textcolor{red}{Human activity recognition (HAR) aims to classify daily human activities based on sensor data and has attracted significant attention in recent years~\cite{lara2012survey,attal2015physical,kulsoom2022review}.
Among various sensing modalities, inertial measurement unit (IMU)-based HAR has been widely adopted for applications such as smart homes~\cite{du2019novel}, healthcare~\cite{schrader2020advanced}, and input interfaces~\cite{hsiao2017design}.
As these applications are increasingly deployed over extended periods~\cite{hiremath2023lifespan}, HAR systems must adapt to evolving user behaviors and newly introduced activity classes~\cite{hiremath2022bootstrapping}.}

\textcolor{red}{A naïve retraining approach using all available data is often impractical due to resource constraints and privacy concerns~\cite{han2021deep}.
Moreover, plasticity-focused methods such as fine-tuning typically suffer from catastrophic forgetting~\cite{french1999catastrophic, mccloskey1989catastrophic, tian2024survey}, where performance on previously learned tasks deteriorates as the model overfits to new data.
Consequently, there is growing interest in continual learning (CL) methods, which enable models to learn incrementally without forgetting prior knowledge~\cite{de2021continual, chen2018lifelong}.
CL methods are generally categorized into three families: regularization-based, replay-based, and parameter isolation methods.}

\textcolor{red}{Within CL, different problem scenarios are defined based on how tasks arrive sequentially.
One widely studied scenario is class-incremental learning (CIL), where the model must learn an increasing number of classes over time without task labels and with no class repetition across tasks (Fig.~\ref{fig:fig1}(A)).
However, real-world systems rarely operate under conditions where classes are completely disjoint across tasks.
Blurry class-incremental learning (B-CIL) relaxes this constraint by allowing partial class overlap between tasks, making it more representative of practical deployment scenarios (Fig.~\ref{fig:fig1}(B)).}
%
\begin{figure}[t]
    \includegraphics[width=\linewidth]{Figs/EMBC_teaser.eps}
    \caption{\textcolor{red}{Overview of (A)~class-incremental learning and (B)~blurry class-incremental learning scenarios.}}
    \label{fig:fig1}
\end{figure}

\textcolor{red}{For IMU-based HAR, previous studies have explored CIL scenarios~\cite{leite2022resource,kann2023evaluation,kwon2021exploring,qiao2024class,adaimi2022lifelong,schiemer2023online,fan2024ts,wu2025ptms}, but most assume that classes are fully disjoint.
The only related work by Zhang et al.~\cite{zhang2021harmi} considers a setting where one new class is added at each task and later reappears once.
However, in real-world scenarios, old and new classes often coexist within the same task. 
To date, no systematic investigation has addressed B-CIL scenarios with varying degree of class overlap.}

\textcolor{red}{In this study, we evaluate nine CL methods under B-CIL scenarios with different levels of class overlap.
Experiments are conducted on two publicly available datasets: the University of California Human Activity Recognition (UCI-HAR) and the University of Southern California Human Activity Dataset (USC-HAD).
Our findings reveal that replay-based methods consistently outperform regularization-based ones and that increasing class overlap improves performance.}

\textcolor{red}{The research questions addressed in this work are as follows:
%
\begin{itemize}
    \item \textbf{RQ1:} Which CL methods are most effective in B-CIL scenarios for IMU-based HAR?
    \item \textbf{RQ2:} How does the degree of class overlap influence the performance of CL methods in B-CIL scenarios?
\end{itemize}
}

\section{Continual Learning Framework and Scenario Definitions}
\textcolor{red}{In a CL scenario, tasks arrive sequentially, and the model $f(\boldsymbol{x}; \theta)$ is incrementally updated upon the arrival of each new task, where $\boldsymbol{x}$ denotes the input data and $\theta$ represents the model parameters.
Formally, the sequence of tasks is defined as $T = \{\tau^{1}, \tau^{2}, ..., \tau^{N}\}$, where each task $\tau^t$ ($t \in \{1, ..., N\}$) is associated with a dataset $D^{t} = \{(x_{i}^{t}, y_{i}^{t}) |y_{i}^{t} \in Y^{t}\}_{i=1}^{N_{t}}$ and a corresponding label space $Y^{t}$.
Each input $x_i^t$ is sampled from a task-specific input domain $\mathcal{X}^t$, and each label $y_i^t$ belongs to the respective label space $Y^t$.
We denote the model before training on task $\tau^{t}$ as $f_{\theta_{t}}$, and the model after optimization as $f_{\theta_{t}^{*}}$.
The CL scenarios considered in this study are defined as follows:
\begin{itemize}
    \item CIL:
    Each task introduces a set of new, mutually exclusive classes. Formally, for any $i \neq j$, the label spaces are disjoint, i.e., $Y^{i} \cap Y^{j} = \varnothing$, indicating that no class is shared between tasks.
    \item B-CIL:
    This scenario relaxes the strict disjoint-class assumption of CIL by allowing partial overlap between the label spaces of tasks. Specifically, for some $i \neq j$, $Y^{i} \cap Y^{j} = \varnothing$, indicating that certain classes may appear in multiple tasks.
    % \item DIL:
    % DIL assumes a fixed label space across all tasks while allowing the input distribution to vary over time.
    % Formally, \( Y^{t} = Y \) for all \( t \in \{1, \ldots, N\} \), meaning all tasks share the same set of classes.
    % However, the input data distributions differ between tasks, i.e., \( P(X^{i}) \neq P(X^{j}) \) for \( i \neq j \).
    % \item CD-CIL: 
    % This represents a more challenging scenario in which both the class distribution and the input distribution change across tasks.
    % Formally, for all \( i \neq j \), the label spaces are disjoint \( (Y^{i} \cap Y^{j} = \varnothing) \), and the input distributions are different \( (P(X^{i}) \neq P(X^{j})) \).
    % \item CD-B-CIL:
    % This combines the characteristics of both B-CIL and CD-CIL.
    % Tasks may share some classes \( (Y^{i} \cap Y^{j} \neq \varnothing \) for some \( i \neq j) \), while the input distributions vary across tasks \( (P(X^{i}) \neq P(X^{j})) \).
\end{itemize}
}


\section{Methods}
\subsection{Backbone Model}
\textcolor{red}{As the backbone model, we employed one-dimensional convolutional neural network (1D-CNN) architecture commonly used in previous class-incremental learning studies~\cite{qiao2024class}.
Specifically, we adopted the architecture proposed in~\cite{qiao2024class}, which consists of four convolutional blocks.
Each block consists of a convolutional layer (kernel size: 5, stride: 1, padding: 2) with output channels of 64, 128, 256, and 128, respectively.
Following the convolutional layer, the block includes a ReLU activation function, a batch normalization layer, and a max-pooling layer (kernel size: 2, stride: 2).}

\subsection{Baselines}
\textcolor{red}{As baselines, we considered two approaches.
The first is a naïve fine-tuning strategy (Naïve), in which the model is incrementally updated on each new task without applying any CL methods, serving as a reference point for catastrophic forgetting.
The second is an offline setting (Offline), where the model is trained using data from all tasks simultaneously.
This represents an upper-bound performance reference, as it assumes full access to the entire dataset across tasks.}

\subsection{Continual Learning Methods}
\textcolor{red}{As the CL methods for comparison, we selected nine representative methods: four regularization-based methods--Learning without Forgetting (LwF)~\cite{li2017learning}, Elastic Weight Consolidation (EWC)~\cite{kirkpatrick2017overcoming}, Memory Aware Synapses (MAS)~\cite{aljundi2018memory}, and Synaptic Intelligence (SI)~\cite{zenke2017continual}--and five replay-based methods--Experienced Replay (ER)~\cite{rolnick2019experience}, Dark Experience Replay (DER)~\cite{lopez2017gradient}, Fast Incremental Classifier and Representation Learning (FastICARL)~\cite{kwon2021fasticarl}, Adversarial Shapley value Experience Replay (ASER)~\cite{shim2021online}, and Generative Replay (GR)~\cite{shin2017continual}.}
% \subsubsection{Regularization-based methods}
% We selected four representative regularization-based CL methods for comparison: Learning without Forgetting (LwF)~\cite{li2017learning}, Elastic Weight Consolidation (EWC)~\cite{kirkpatrick2017overcoming}, Memory Aware Synapses (MAS)~\cite{aljundi2018memory}, and Synaptic Intelligence (SI)~\cite{zenke2017continual}.
% These methods mitigate catastrophic forgetting by incorporating additional loss terms that constrain updates to important model parameters based on knowledge acquired from previous tasks.

% LwF~\cite{li2017learning} mitigates catastrophic forgetting by leveraging a knowledge distillation (KD) loss as a regularization term. 
% Specifically, LwF encourages the current model $f_{\boldsymbol{\theta}}$ to retain knowledge from previous tasks by minimizing the discrepancy between its output and that of a frozen model $f_{\boldsymbol{\theta}^*}$--trained on earlier tasks--when evaluated on the current task data.
% This is achieved in conjunction with minimizing the standard cross-entropy (CE) loss $\mathcal{L}_{\text{CE}}$ for the current task labels.
% In this study, we adopt the adaptive-weight variant of LwF, where the contribution of the CE and KD losses varies depending on the number of tasks seen so far.
% The total loss function for task $\tau^t$ is defined as:

% \begin{eqnarray}
%     \mathcal{L} &=& \frac{1}{t} \cdot \mathcal{L}_{\text{CE}} + \left(1 - \frac{1}{t} \right) \cdot \lambda \cdot \mathcal{L}_{\text{KD}^{t}}  \\
%     \mathcal{L}_{\mathrm{KD}}^{t} &=& - \sum_{c \in Y^{1:t-1}} \tilde{y}^{t-1}_{c} \log \tilde{y}^{t}_{c} \\
%     \tilde{y}^{t-1}_{i} &=& \mathrm{Softmax}(f_{\theta_{t-1}^{*}}(x_{i}^{t}) / T), \nonumber \\
%     \tilde{y}^{t}_{i} &=& \mathrm{Softmax}(f_{\theta_{t}}(x_{i}^{t}) / T) 
% \end{eqnarray}
% where $T$ is the temperature parameter used for smoothing the output distributions, and is set to 2 following~\cite{mai2022online}.

% EWC~\cite{kirkpatrick2017overcoming} addresses catastrophic forgetting by selectively constraining updates to parameters deemed important for previously learned tasks.
% The importance of each parameter is quantified using the diagonal elements of the Fisher Information Matrix (FIM), which captures the sensitivity of the loss with respect to each parameter.
% The total loss function for task $\tau^{t}$ is defined as:
% %
% \begin{equation}
%  \mathcal{L} = \mathcal{L}_{\mathrm{CE}} + \frac{\lambda}{2} \sum_{c \in Y^{1:t}} F_c (\boldsymbol{\theta}_{t, c} - \boldsymbol{\theta}_{t-1, c}^{*})^2,
% \end{equation}
% %
% where $F_c$ represents the importance score of parameter $\theta_c$ computed from previous tasks, $\boldsymbol{\theta}_{t}$ is the current model parameter vector, and $\boldsymbol{\theta}_{t-1}^{*}$ is the optimized parameter vector from the previous task.
% The regularization term penalizes deviations from the previously important parameters, thereby preserving prior knowledge.

% MAS~\cite{aljundi2018memory} is a regularization-based approach inspired by neuroplasticity, which quantifies the importance of each parameter based on its contribution to the model's output.
% Unlike EWC, which relies on the FIM, MAS estimates importance by computing the magnitude of the gradients of the model outputs with respect to the parameters.
% The total loss function for task $\tau^t$ is given by:
% \begin{eqnarray}
%     \mathcal{L} &=& \mathcal{L}_{\mathrm{CE}} + {\lambda} \sum_{c \in Y^{1:t}} \Omega_c (\boldsymbol{\theta}_{t, c} - \boldsymbol{\theta}_{t-1, c}^{*})^2,\\
%     \Omega_c  &=& \frac{1}{N} \sum_{i=1}^{N} || \frac{\partial (f (x_i; \theta))}{\partial \theta_c}||,
% \end{eqnarray}
% where $\Omega_c$ denotes the estimated importance of parameter $\theta_c$ and is computed as the average norm of the gradients over $N$ samples.
% This regularization discourages large updates to parameters that significantly influence the model's predictions, thereby preserving knowledge acquired from previous tasks.

% SI~\cite{zenke2017continual} addresses catastrophic forgetting by penalizing changes to parameters that were important for solving previous tasks.
% Similar to MAS, SI estimates the importance of each parameter based on its historical contribution to reducing the loss. Unlike methods that rely on second-order information, SI computes importance online during training by tracking the product of the gradient and the parameter update.
% The loss function for task $\tau^t$ is defined as:
% %
% \begin{eqnarray}
%     \mathcal{L} &=& \mathcal{L}_{\mathrm{CE}} + {\lambda} \sum_{c \in Y^{1:t}} \omega_c (\boldsymbol{\theta}_{t, c} - \boldsymbol{\theta}_{t-1, c}^{*})^2,
% \end{eqnarray}
% %
% where $\omega_c$ represents the estimated importance of parameter $\theta_c$ and is computed as:
% %
% \begin{eqnarray}
%     \omega_c = \sum_{t} \frac{g_c^{(t)} \cdot \Delta \theta_c^{(t)}}{(\Delta \theta_c^{(t)})^2 + \epsilon},
% \end{eqnarray}
% %
% with $g_c^{(t)}$ denoting the gradient of the loss with respect to parameter $\theta_c$ at training step $t$, $\Delta \theta_c^{(t)}$ the corresponding parameter update, and $\epsilon$ a small constant added for numerical stability.
% This approach allows SI to adaptively preserve important knowledge while enabling learning on new tasks.


% \subsubsection{Replay-based methods}
% We selected five representative replay-based methods for comparison: Experienced Replay (ER)~\cite{rolnick2019experience}, Dark Experience Replay (DER)~\cite{lopez2017gradient}, Fast Incremental Classifier and Representation Learning (FastICARL)~\cite{kwon2021fasticarl}, Adversarial Shapley value Experience Replay (ASER)~\cite{shim2021online}, and Generative Replay (GR)~\cite{shin2017continual}.

% ER~\cite{rolnick2019experience} is a fundamental replay-based method in which a fixed-size memory buffer is used to store samples from previous tasks.
% During training on a new task, the model is updated using a combination of current task data and replayed samples drawn from the memory buffer.
% In our implementation, we adopted random sampling strategies for both memory retrieval and memory update, consistent with the original ER framework.
% The overall loss function used in task $\tau^t$ is defined as:
% %
% \begin{eqnarray}
%     \mathcal{L} 
%     = \mathcal{L}_{\mathrm{CE}} 
%     + \mathcal{L}_{\mathrm{Buffer}},
% \end{eqnarray}
% %
% where $\mathcal{L}_{\mathrm{Buffer}}$ denotes the cross-entropy loss computed on the replayed samples from the memory buffer.

% DER~\cite{lopez2017gradient} extends the standard ER framework by incorporating a KD loss.
% Like ER, DER employs random sampling strategies for both memory retrieval and update.
% In addition to the CE loss on buffered samples, DER introduces a distillation loss that encourages the current model to mimic the output of the previous model on the same buffered inputs.
% The overall loss function is defined as:
% %
% \begin{eqnarray}
%     \mathcal{L} = 
%     \mathcal{L}_{\mathrm{CE}}
%     + \frac{1}{2} \cdot \mathcal{L}_{\mathrm{Buffer}}
%     + \frac{1}{2} \cdot \mathcal{L}_{\mathrm{Buffer_{KD}}},
% \end{eqnarray}
% %
% where $\mathcal{L}_{\mathrm{Buffer}}$ denotes the CE loss on memory samples, and $\mathcal{L}_{\mathrm{Buffer_{KD}}}$ represents the KD loss computed with respect to the soft targets (i.e., logits or softmax outputs) stored in the buffer.

% FastICARL~\cite{kwon2021fasticarl} is a computationally efficient variant of the original iCaRL method~\cite{rebuffi2017icarl}, a class-incremental learning approach that integrates exemplar memory with a nearest-mean-of-exemplars classifier.
% FastICARL addresses the high computational cost of iCaRL by replacing its $k$-nearest neighbor ($k$-NN)–based exemplar selection with a more efficient max-heap–based strategy.
% The model is trained by minimizing the standard CE loss over the combined set of current task data and stored exemplars, along with a KD loss:
% %
% \begin{equation}
%     \mathcal{L} = \mathcal{L}_{\mathrm{CE}} + \lambda \cdot \mathcal{L}_{\mathrm{Buffer_{KD}}}.
% \end{equation}

% ASER~\cite{shim2021online} is a replay-based CL method that prioritizes samples in the memory buffer based on their Shapley values.
% This approach aims to identify and replay samples that are not only representative of previous classes but also likely to interfere with the learning of new incoming data.
% ASER achieves this by computing adversarial Shapley values (ASV), which quantify the trade-off between cooperative and adversarial contributions of each sample to model performance under distributional shifts or adversarial scenarios.
% %
% \begin{equation}
%     \text{ASV}(i) = \frac{1}{|S_{\text{sub}}|} \sum_{j \in S_{\text{sub}}} s_j(i) 
%     - \frac{1}{|B|} \sum_{k \in B} s_k(i),
% \end{equation}
% %
% where $S_{\text{sub}}$ denotes a class-balanced subset of the replay buffer, $B$ represents the current memory buffer, and $s_j(i)$ is the Shapley value of sample $i$ with respect to sample $j$.
% The first term captures the cooperative contribution of sample $i$ to the subset $S_{\text{sub}}$, while the second term measures its interference with the current batch $B$.
% Due to the computational complexity of exact Shapley value estimation, ASV is typically approximated using $k$-nearest neighbors.
% In this study, we follow~\cite{qiao2024class} and set $k=3$.

% GR~\cite{shin2017continual} is a replay-based CL method that synthesizes pseudo-samples resembling previous task data using a generative model.
% These generated samples are then replayed alongside new task data during training to mitigate catastrophic forgetting.
% In GR, a generator $g_\phi $ and a learner $f_\theta$ are trained jointly.
% At each task step, the generator from the previous task generates synthetic samples, while the learner assigns pseudo-labels to these samples.
% The model is then trained on a combination of the generated data and the current task data.
% After updating the learner, the generator is also retrained to reflect the updated knowledge.
% In this study, we adopt TimeVAE~\cite{desai2021timevae} as the generative model, which is well-suited for sequential and time-series data generation.


\section{Materials}
%
\begin{figure*}[t]
    \includegraphics[width=\linewidth]{Figs/EMBC_Scenarios.eps}
    \caption{\textcolor{red}{Three configurations of CIL and B-CIL scenarios for two datasets: (A)~UCI-HAR and (B)~USC-HAD datasets.}}
    \label{fig:scenarios}
\end{figure*}
%
\textcolor{red}{We used two publicly available datasets; UCI-HAR and USC-HAD.}

\textcolor{red}{UCI-HAR~\cite{anguita2013public} comprises data collected from 30~subjects using a waist-mounted smartphone (Samsung Galaxy S II) equipped with inertial sensors.
The sampling rate was set to 50~Hz.
The dataset includes six activities: (1)~walking, (2)~walking upstairs, (3)~walking downstairs, (4)~sitting, (5)~standing, and (6)~lying.
Each sample contains nine feature dimensions, consisting of 3-axis total acceleration, 3-axis estimated body acceleration, and 3-axis angular velocity.
Sensor signals were segmented using sliding windows of 2.56~s and 50~\% overlap, resulting in window shapes of $128{}\times{}9$.}

\textcolor{red}{USC-HAD~\cite{zhang2012usc} comprises data collected from 14~subjects using a single IMU sensor (MotionNode) positioned on the front right side of the body.
The sampling rate was set to 100~Hz.
The dataset includes 12~activities: (1)~walking forwards, (2)~walking left, (3)~walking right, (4)~walking upstairs, (5)~walking downstairs, (6)~running forwards, (7)~jumping, (8)~sitting, (9)~standing, (10)~sleeping, (11)~elevator up, and (12)~elevator down.
Each sample has six feature dimensions, consisting of 3-axis acceleration and 3-axis angular velocity.
Sensor signals were segmented using sliding windows of 1.28~s and 50\% overlap, resulting in window shapes of $128{}\times{}6$.}


\section{Experiments}
\textcolor{red}{In this study, we evaluated two B-CIL scenarios (B-CIL1 and B-CIL2), and compared them against the standard CIL scenario.
Fig.~\ref{fig:scenarios} illustrates the detailed configurations of these scenarios for the two datasets: (A)~UCI-HAR and (B)~USC-HAD.}

\subsection{Class Allocation Across Tasks}
\textcolor{red}{In this study, the number of tasks was set to three.
In the CIL scenario, classes from each dataset were evenly distributed across tasks to ensure that no class overlapped between tasks.
Consequently, for the UCI-HAR dataset, which contains six classes, each task was assigned two classes, while for the USC-HAD dataset, which contains twelve classes, each task was assigned four classes.}

\textcolor{red}{In the B-CIL1 scenario, compared to CIL, one class from the Task~1 was also assigned to Tasks~2 and~3.
As a result, the number of classes per task was [2, 3, 3] for UCI-HAR and [4, 5, 5] for USC-HAD.}

\textcolor{red}{In the B-CIL2 scenario, two classes from Task~1 were shared with both Tasks~2 and~3. 
Therefore, the number of classes per task was [2, 4, 4] for UCI-HAR and [4, 6, 6] for USC-HAD.}

\textcolor{red}{The order of classes within each task was determined randomly.
The split into training, validation, and test sets was also performed randomly, with 60\% of the data used for training, 20\% for validation, and 20\% for testing.}

\subsection{Evaluation Metrics}
\textcolor{red}{We employed three evaluation metrics: (1)~Final Average Accuracy, (2)~Final Average Forgetting, and (3)~Average Learning Accuracy.
Let $a_{i, j}$ denote the average classification accuracy on Task~$j$ after the model has been trained on Task~$i$, where $j \leqq i$. The total number of tasks is $T$.}

\subsubsection{Final Average Accuracy}
\textcolor{red}{This metric represents the average accuracy across all tasks after the model has completed training on all tasks:
%
\begin{equation}
    \mathcal{A}_{T} = \frac{1}{T} \sum_{i=1}^{T} a_{T, i}.
\end{equation}
}

\subsubsection{Final Average Forgetting}
\textcolor{red}{This metric quantifies the average decrease in accuracy on previous tasks due to learning subsequent tasks:
%
\begin{equation}
    \mathcal{F}_{T} = \frac{1}{T-1} \sum_{j=1}^{T} \max_{k \in \{1, ..., T-1\}} (a_{k, j} - a_{i, j}) (j < i).
\end{equation}
}

\subsubsection{Average Learning Accuracy}
\textcolor{red}{This metric measures the average accuracy on the current task immediately after training on that task:
%
\begin{equation}
    \mathcal{A}_{\mathrm{cur}} = \frac{1}{T} \sum_{i=1}^{T} a_{i, i}.
\end{equation}
}

\subsection{Learning Protocol}
% 全体的にここの記述が再現しにくそうな印象
\textcolor{red}{For the training configuration, Cross-Entropy Loss was used as the objective function, and the OneCycle learning rate scheduler was employed.
The model was trained for 100~epochs.
Early stopping was applied to the regularization-based methods and baseline method, with a patience parameter of 5.}

\textcolor{red}{For all training methods, the batch size, maximum learning rate, and learning rate adjustment strategy were optimized using grid search.
The search range for batch size was [32, 64, 128], and for the initial learning rate, it was [0.01, 0.001, 0.0001].
For the learning rate adjustment strategy, three approaches were evaluated: (1)~reducing the learning rate by a factor of 0.1 every 10~epochs, (2)~reducing it by a factor of 0.1 every 25~epochs, and (3)~using only the scheduler without manual adjustments.}

\textcolor{red}{Hyperparameter optimization was performed exclusively using validation data from Task~1.}
\textcolor{blue}{The final hyperparameters employed in this study are summarized in Table~\ref{tab:hyperparameter}.}
% この部分要確認

\subsection{Implementation}
\textcolor{red}{All methods were implemented in Python 3.10.10 and PyTorch 1.13.1.
Experiments were conducted on a machine running Ubuntu 22.04, equipped with four NVIDIA L40S GPUs.}
% In the UCI-HAR dataset, out of the data from eight participants (excluding the test data for Task 1), data from two participants were selected using 4-fold cross-validation. 
% In the USC-HAD dataset, out of the data from three participants (excluding the test data for Task 1), data from one participant were selected using 3-fold cross-validation.


\section{Results}
%
\begin{table}[t]
    \centering
    \caption{\textcolor{blue}{Hyperparameters employed in this study for CL methods.}}
    \resizebox{0.95\linewidth}{!}{
    \begin{tabular}{c  c  c }
       \toprule
       Method & Hyperparameter & Search range\\ 
       \midrule
        LwF & $\lambda$ & 1, 0.1, 0.01, 0.001, 0.0001 \\
        \midrule
        EWC & $\lambda$ & 0.01, 0.001, 0.0001 \\
        \midrule
        MAS & $\lambda$ & 0.001, 0.0001, 0.00001 \\
        \midrule
        SI & $\lambda$ & 0.01, 0.001, 0.0001 \\
        \midrule
        ASER & Number of sample saved in buffer per class & 2, 4 \\
        \midrule
        \multirow{2}{*}{GR} & Learning rate for generator & 0.001, 0.0001\\
        & Weight for reconstruction loss & 0.01, 0.1, 1, 10 \\
    %    \midrule
    %    \multirow{3}{*}{USC-HAD} & CD-CIL       & 2, 1, 4, 0   & 9, 11, 10, 8   & 7, 5, 6, 3 \\
       \bottomrule
    \end{tabular}
    }
    \label{tab:hyperparameter}
\end{table}
%
\begin{table*}[!ht]
    \centering
    \caption{\textcolor{red}{Results of baseline and CL methods on UCI-HAR and USC-HAD datasets.}}
    \begin{tabular}{c c c cc|cccc|ccccc}
        \toprule
        Dataset & Metrics & Scenario & Naïve & Offline & LwF & EWC & MAS & SI & ER & DER & ASER & FastICARL & GR \\
        \midrule

        % === A_T ===
        \multirow{6}{*}{$\mathcal{A}_T$} & \multirow{3}{*}{UCI-HAR} & CIL   & 33.04 & 98.00 & 34.35 & 35.36 & 40.47 & 34.35 & 70.97 & 66.62 & \textbf{96.76} & 66.44 & 36.34 \\
                                          &                            & B-CIL1& 61.16 & N.A. & 65.49 & 63.52 & 70.58 & 63.91 & 89.17 & 88.74 & \textbf{98.86} & 89.69 & 62.70 \\
                                          &                            & B-CIL2& 82.52 & N.A. & 83.15 & 83.44 & 84.60 & 80.56 & 96.00 & 92.83 & \textbf{99.40} & 95.64 & 83.49 \\
        \cmidrule(lr){2-14}
                                          & \multirow{3}{*}{USC-HAD}  & CIL   & 32.82 & 91.43 & 38.72 & 37.15 & 38.69 & 33.64 & 76.94 & 64.42 & \textbf{94.11} & 73.13 & 29.23 \\
                                          &                            & B-CIL1& 49.07 & N.A. & 57.11 & 54.75 & 54.59 & 51.55 & 82.58 & 72.32 & \textbf{95.67} & 80.93 & 54.49\\
                                          &                            & B-CIL2& 58.04 & N.A. & 62.34 & 65.18 & 60.79 & 57.17 & 86.78 & 77.30 & \textbf{96.28} & 78.67 & 50.85\\
        \midrule

        % === F_T ===
        \multirow{6}{*}{$\mathcal{F}_T$} & \multirow{3}{*}{UCI-HAR} & CIL   & 99.15 & N.A. & 91.35 & 95.44 & 85.20 & 88.95 & 19.80 & 17.06 & \textbf{4.24} & 29.51 & 91.81\\
                                          &                           & B-CIL1& 56.78 & N.A. & 46.46 & 53.12 & 39.78 & 46.72 & 7.38  & 5.43  & \textbf{1.27} & 8.06  &  54.35\\
                                          &                           & B-CIL2& 23.98 & N.A. & 19.42 & 22.66 & 18.24 & 20.89 & 2.81  & 3.53  & \textbf{0.57} & 2.97  & 22.92\\
        \cmidrule(lr){2-14}
                                          & \multirow{3}{*}{USC-HAD} & CIL   & 98.62 & N.A.  & 77.66 & 88.55 & 78.80 & 70.18 & 12.36 & 9.16  & \textbf{3.94} & 14.32 & 65.99 \\
                                          &                           & B-CIL1& 72.65 & N.A. & 55.73 & 62.03 & 55.33 & 50.61 & 8.38  & 6.32  & \textbf{2.23} & 11.47 & 52.65 \\
                                          &                           & B-CIL2& 58.60 & N.A. & 45.21 & 44.56 & 46.31 & 41.62 & 5.35  & 4.90  & \textbf{1.48} & 16.43 & 44.3\\
        \midrule

        % === A_cur ===
        \multirow{6}{*}{$\mathcal{A}_{cur}$} & \multirow{3}{*}{UCI-HAR} & CIL   & 99.14 & N.A.& 96.75 & 98.98 & 97.27 & 93.65 & 81.37 & 76.74 & \textbf{99.58} & 85.54 & 97.55\\
                                             &                           & B-CIL1& 99.01 & N.A. & 96.46 & 98.84 & 97.10 & 94.06 & 93.31 & 92.03 & \textbf{99.69} & 95.03 & 98.44 \\
                                             &                           & B-CIL2& 98.40 & N.A. & 96.08 & 98.46 & 96.74 & 94.48 & 97.78 & 95.06 & \textbf{99.67} & 97.52 & 98.58 \\
        \cmidrule(lr){2-14}
                                             & \multirow{3}{*}{USC-HAD} & CIL   & \textbf{97.90} &  N.A.& 90.49 & 96.18 & 91.22 & 80.43 & 85.01 & 68.95 & 96.73 & 82.64 & 73.23\\
                                             &                           & B-CIL1& \textbf{97.50} & N.A. & 94.27 & 96.10 & 91.48 & 85.21 & 88.07 & 75.81 & 97.15 & 88.53 & 89.59\\
                                             &                           & B-CIL2& 97.11 & N.A. & 92.48 & 94.89 & 91.67 & 84.80 & 90.26 & 79.71 & \textbf{97.15} & 89.61 & 80.39\\
        \bottomrule
    \end{tabular}
    \label{tbl:main_results}
\end{table*}

% \begin{table*}[!ht]
%     \centering
%     \caption{Results of the continual learning methods in the UCI-HAR and USC-HAD datasets (order: FF)}
%     \begin{tabular}{c c c cc|cccc|ccccc}
%         \toprule
%         Dataset & Metrics & Scenario & Naive & Offline & LwF & EWC & MAS & SI & ER & ASER & DER & FastICARL & GR \\
%         \midrule

%         % === A_T ===
%         \multirow{6}{*}{$\mathcal{A}_T$} & \multirow{3}{*}{UCI-HAR} & CIL   & 33.04 & N.A. & 34.35 & 35.36 & 40.47 & 34.35 & 70.97 & 66.62 & \textbf{96.76} & 66.44 & 36.34 \\
%                                           &                            & B-CIL1& 61.16 & N.A. & 65.49 & 63.52 & 70.58 & 63.91 & 89.17 & 88.74 & \textbf{98.86} & 89.69 & 62.70 \\
%                                           &                            & B-CIL2& 82.52 & N.A. & 83.15 & 83.44 & 84.60 & 80.56 & 96.00 & 92.83 & \textbf{99.40} & 95.64 & 83.49 \\
%         \cmidrule(lr){2-14}
%                                           & \multirow{3}{*}{USC-HAD}  & CIL   & 32.82 & N.A. & 38.72 & 37.15 & 38.69 & 33.64 & 76.94 & 64.42 & \textbf{94.11} & 73.13 & 29.23 \\
%                                           &                            & B-CIL1& 49.07 & N.A. & 57.11 & 54.75 & 54.59 & 51.55 & 82.58 & 72.32 & \textbf{95.67} & 80.93 & N.A. \\
%                                           &                            & B-CIL2& 58.04 & N.A. & 62.34 & 65.18 & 60.79 & 57.17 & 86.78 & 77.30 & \textbf{96.28} & 78.67 & N.A. \\
%         \midrule

%         % === F_T ===
%         \multirow{6}{*}{$\mathcal{F}_T$} & \multirow{3}{*}{UCI-HAR} & CIL   & 99.15 & N.A. & 91.35 & 95.44 & 85.20 & 88.95 & 19.80 & 17.06 & \textbf{4.24} & 29.51 & N.A. \\
%                                           &                           & B-CIL1& 56.78 & N.A. & 46.46 & 53.12 & 39.78 & 46.72 & 7.38  & 5.43  & \textbf{1.27} & 8.06  & N.A. \\
%                                           &                           & B-CIL2& 23.98 & N.A. & 19.42 & 22.66 & 18.24 & 20.89 & 2.81  & 3.53  & \textbf{0.57} & 2.97  & N.A. \\
%         \cmidrule(lr){2-14}
%                                           & \multirow{3}{*}{USC-HAD} & CIL   & 98.62 & N.A. & 77.66 & 88.55 & 78.80 & 70.18 & 12.36 & 9.16  & \textbf{3.94} & 14.32 & N.A. \\
%                                           &                           & B-CIL1& 72.65 & N.A. & 55.73 & 62.03 & 55.33 & 50.61 & 8.38  & 6.32  & \textbf{2.23} & 11.47 & N.A. \\
%                                           &                           & B-CIL2& 58.60 & N.A. & 45.21 & 44.56 & 46.31 & 41.62 & 5.35  & 4.90  & \textbf{1.48} & 16.43 & N.A. \\
%         \midrule

%         % === A_cur ===
%         \multirow{6}{*}{$\mathcal{A}_{cur}$} & \multirow{3}{*}{UCI-HAR} & CIL   & 99.14 & N.A. & 96.75 & 98.98 & 97.27 & 93.65 & 81.37 & 76.74 & \textbf{99.58} & 85.54 & N.A. \\
%                                              &                           & B-CIL1& 99.01 & N.A. & 96.46 & 98.84 & 97.10 & 94.06 & 93.31 & 92.03 & \textbf{99.69} & 95.03 & N.A. \\
%                                              &                           & B-CIL2& 98.40 & N.A. & 96.08 & 98.46 & 96.74 & 94.48 & 97.78 & 95.06 & \textbf{99.67} & 97.52 & N.A. \\
%         \cmidrule(lr){2-14}
%                                              & \multirow{3}{*}{USC-HAD} & CIL   & \textbf{97.90} & N.A. & 90.49 & 96.18 & 91.22 & 80.43 & 85.01 & 68.95 & 96.73 & 82.64 & N.A. \\
%                                              &                           & B-CIL1& \textbf{97.50} & N.A. & 94.27 & 96.10 & 91.48 & 85.21 & 88.07 & 75.81 & 97.15 & 88.53 & N.A. \\
%                                              &                           & B-CIL2& 97.11 & N.A. & 92.48 & 94.89 & 91.67 & 84.80 & 90.26 & 79.71 & \textbf{97.15} & 89.61 & N.A. \\
%         \bottomrule
%     \end{tabular}
%     \label{tbl:main_results}
% \end{table*}
% Table \ref{tbl:main_results_UCI_HAR} and Table \ref{tbl:main_results_USC_HAD} 
\textcolor{red}{Table~\ref{tbl:main_results} presents the performance of the baseline and CL methods on the UCI-HAR and USC-HAD datasets.
When comparing the three scenarios across both datasets, CIL consistently achieved the lowest Final Average Accuracy $\mathcal{A}_{T}$, followed by B-CIL1, while B-CIL2 yielded the highest accuracy--except for FastICARL and GR on USC-HAD.
Regarding Final Average Forgetting $\mathcal{F}_{T}$, CIL exhibited the greatest degree of forgetting.
As the number of overlapping classes increased, forgetting decreased, with B-CIL2 showing the lowest forgetting (except for FastICARL on USC-HAD).
Average Learning Accuracy $\mathcal{A}_{\mathrm{cur}}$ varied across methods without a clear trend.
Overall, these results indicate that increasing the number of overlapping classes mitigates forgetting and improves final accuracy.}

% この段落、Baselineとの比較ということでOfflineについても触れてほしい
\textcolor{red}{We compared the Final Average Accuracy $\mathcal{A}_{T}$ of CL methods against baselines (i.e., Naïve and Offline).}
\textcolor{blue}{In the CIL scenario, most CL methods achieved higher accuracy than Naïve, except for GR on USC-HAD.
In the B-CIL1 scenario, all CL methods outperformed Naïve.
In the B-CIL2 scenario, CL methods generally surpassed Naïve, with the exception of GR on USC-HAD and SI on both UCI-HAR and USC-HAD.
Overall, across CIL and scenarios with task overlap, most CL methods tend to outperform the Naive baseline.}

\textcolor{red}{Among the evaluated CL methods, most replay-based methods outperformed regularization-based methods across both datasets and all scenarios, with exception of GR.
For the four replay-based methods (ER, DER, ASER, and FastICARL), the average Final Average Accuracy $\mathcal{A}_{T}$ was 75.20 in CIL, 91.62 in B-CIL1, and 95.97 in B-CIL2 for UCI-HAR, and 77.15 in CIL, 82.88 in B-CIL1, and 84.76 in B-CIL2 for USC-HAD.
In contrast, for the regularization-based methods (LwF, EWC, MAS, and SI), the average Final Average Accuracy was 36.13 in CIL, 70.04 in B-CIL1, and 82.94 in B-CIL2 for UCI-HAR, and 37.05 in CIL, 55.40 in B-CIL1, and 61.37 in B-CIL2 for USC-HAD.
Among the replay-based methods, ASER achieved the highest Final Average Accuracy across all scenarios for both UCI-HAR and USC-HAD.}

% ここ少しDiscussionっぽい
\textcolor{red}{When comparing UCI-HAR and USC-HAD, the CIL scenario yields higher accuracy on USC-HAD for methods such as LwF, EWC, ER, and FastICARL.
In contrast, B-CIL1 and B-CIL2 scenarios consistently favor UCI-HAR.
This difference is attributable to the greater class-level overlap in UCI-HAR: B-CIL1 includes 1 of 6~classes (16.7\%) compared to 1 of 12 (8.3\%) in USC-HAD, while B-CIL2 includes 2 of 6~classes (33.3\%) compared to 2 of 12 (16.7\%).
Increased overlap reinforces previously learned representations, thereby reducing catastrophic forgetting.}


\section{Discussion}
\subsection{Answers to Research Questions}
\subsubsection{RQ1: Which CL methods are most effective in B-CIL scenarios for IMU-based HAR?}
\textcolor{red}{Across both UCI-HAR and USC-HAD datasets, replay-based methods consistently outperformed regularization-based methods in B-CIL scenarios. 
This observation is consistent with prior findings in CIL scenarios~\cite{qiao2024class}, which also reported the superiority of replay-based methods.
Among the replay-based methods, ASER achieved the highest Final Average Accuracy across all B-CIL scenarios and both datasets.}

\subsubsection{RQ2: How does the degree of class overlap influence the performance of CL methods in B-CIL scenarios?}
\textcolor{red}{Increasing the degree of class overlap in B-CIL scenarios had a positive impact on the performance of CL methods.
As the number of overlapping classes increased from B-CIL1 to B-CIL2, most methods exhibited a notable reduction in forgetting and an improvement in Final Average Accuracy.
Furthermore, the improvement was more pronounced when the proportion of overlapping classes relative to the total number of classes was higher, reinforcing the benefit of overlap in mitigating catastrophic forgetting.}

\subsection{Limitations and Future Work}
\subsubsection{Expansion to other datasets and methods}
\textcolor{red}{This study evaluated the performance of CL methods in B-CIL scenarios using two publicly available datasets: UCI-HAR and USC-HAD.
A key limitation is that the experiments were restricted to these two datasets.
In future work, we plan to extend the evaluation to additional human activity datasets to investigate how factors such as the number and order of overlapping classes influence model accuracy.}

\textcolor{red}{To the best of our knowledge, no previous studies have applied the B-CIL scenarios to time-series data.
While CIL has been explored for IMU data and other time-series modalities, such as surface electromyogram (sEMG) signals~\cite{kanoga2024deep}, the blurry setting remains unexplored.
Future research will aim to generalize the B-CIL framework to other types of time-series data.}

\textcolor{red}{In this study, we focused on four regularization-based methods (LwF, EWC, MAS, and SI) and five replay-based methods (ER, ASER, DER, FastICARL, and GR).
Parameter isolated methods, such as Progressive Neural Networks (PNN)~\cite{rusu2016progressive}, were not considered. 
Incorporating such methods into future experiments will provide a more comprehensive comparison of CL strategies under B-CIL scenarios.}

\subsubsection{More various blurry scenarios}
\textcolor{red}{In this study, the model was trained and updated using batch learning.
However, in real-world applications, data often arrives in a streaming format.
As future work, it is essential to evaluate the performance of online CL~\cite{schiemer2023online}, where the model is updated incrementally as data stream in, under B-CIL scenarios.}

\textcolor{red}{Furthermore, this study applied the Blurry problem setting exclusively to CIL.
In contrast, Blurry settings can also be extended to Domain-Incremental Learning (Domain-IL)\cite{matteoni2022continual,kann2024cross}, where the domain changes rather than the classes.
Future research will focus on adapting the Blurry problem setting to Domain-IL and assessing its effectiveness.}


\section{Conclusion}
\textcolor{red}{This study introduced the B-CIL scenario for IMU-based HAR, addressing a previously overlooked challenge in continual learning.
Through comprehensive experiments on two benchmark datasets, we demonstrated that replay-based methods are particularly effective in mitigating catastrophic forgetting under the B-CIL scenario.
Our analysis further highlights the positive impact of class overlap on model stability and accuracy, offering new insights into designing robust incremental learning systems for real-world HAR applications.
Future work will focus on extending these findings to diverse datasets and exploring online and streaming settings to better reflect practical deployment scenarios.}

\bibliographystyle{IEEEtran}
\bibliography{Reference}
\end{document}